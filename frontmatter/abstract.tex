

\chapter{Abstract}

\ac{pprl} enables dataset integration across institutional boundaries without disclosing \ac{pii} through similarity-preserving encoding schemes such as \ac{bf}, \ac{tmh}, and \ac{tsh}.
However, these schemes are vulnerable to inference-based attacks that exploit structural properties of encoded data.
This thesis introduces the \ac{dea}, a novel two-stage approach that extends the capabilities of the \ac{gma} by enabling the re-identification of individuals who were previously unmatched.

The \ac{dea} leverages \ac{ann} to learn statistical relationships between encoded representations and underlying plaintext n-grams.
Training on re-identified records from a preceding \ac{gma}, the \ac{dea} generalizes to infer n-gram distributions for previously unmatched entries.
The attack is framed as multi-label classification, where the \ac{ann} predicts n-gram presence in original identifiers based on encoded representations.
Three reconstruction strategies are explored: graph-based reconstruction, dictionary-based fuzzy matching, and \ac{llm}-based semantic reconstruction.

Extensive experiments using synthetic and semi-realistic datasets evaluate the \ac{dea} across varying dataset sizes, encoding schemes, and overlaps.
Results show that the \ac{dea} significantly outperforms frequency-based baselines, achieving over 28\% re-identification in favorable configurations and highlighting substantial privacy risks.

Key findings reveal that \ac{bf} and \ac{tsh} expose recurring patterns exploitable through \ac{ann}s, while \ac{tmh} shows highest resilience though still permitting partial reconstruction.
A non-linear correlation exists between prediction quality and re-identification success, with attacks becoming increasingly effective as \ac{ann} F1-scores exceed 0.9.

Thiss thesis contributes a scalable attack model highlighting structural vulnerabilities in widely deployed \ac{pprl} schemes, underscoring the need to reassess current protocols and develop encoding mechanisms that resist statistical inference.
