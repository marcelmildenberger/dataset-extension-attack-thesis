\chapter{Introduction}  \label{sec:introduction}

% Hypotheses:
% DEAs can reconstruct plaintexts with high accuracy even for non-intersecting records by leveraging patterns in the encoded data.
%
% key research questions:
% How can GMAs be enhanced to exploit additional information beyond simple intersections?
% What are the theoretical and practical limits of DEAs in reconstructing plaintext from encoded databases?
% How do different encoding parameters (e.g., Bloom Filter size, number of hash functions, n-gram size) affect the success rate of DEAs?


Linking data and records is an important component of research, software development and software projects.
The primary reason for integrating data from different sources is to gain richer, more comprehensive insights about the same entity.
Initially, deterministic record linkage, which relies on exact matches between predefined identifiers such as unique IDs, was the main method used in early linkage techniques.
However, deterministic approaches often fail in real-world scenarios where data may suffer from inconsistent formatting, typographical errors or missing values, making exact matches impossible \cite{herzog2007data}.

The introduction of a probabilistic framework for record linkage by Fellegi et al. in 1969 \cite{fellegi1969theory} marked a significant advance in overcoming these limitations.
Their work, \enquote{A Theory for Record Linkage} \cite{fellegi1969theory}, proposed a statistical method for linking records across datasets by calculating the probability that two records refer to the same entity, even when there are inconsistencies in the data.
Their approach evaluates common attributes and assigns weights based on the likelihood of a match.
By accounting for discrepancies in real-world data, the so-called Fellegi-Sunter model has become a fundamental methodology for data linkage, particularly in heterogeneous and distributed data environments where traditional deterministic methods fall short \cite{fellegi1969theory}.

Such a probabilistic approach to record linkage is important in sectors such as healthcare and social sciences, where data is often distributed across multiple institutions or sources and lacks unique identifiers.
In these fields, the ability to integrate datasets is essential for gaining insights and improving outcomes.
In the United States, for example, the healthcare system is highly fragmented, consisting of numerous independent entities such as hospitals, clinics, insurance companies, public health agencies, and research institutions.
Each of these organisations collects and stores patient data independently, often using different systems and, more importantly, different formats.
This fragmentation creates significant challenges when trying to track patient outcomes, monitor disease outbreaks, or evaluate the effectiveness of treatments across populations.
Effective data linkage can bridge these gaps by linking records that refer to the same individual across multiple datasets.
This integration is critical for tasks such as epidemiological research, public health surveillance, personalised medicine and healthcare quality improvement \cite{pathak2024privacy, vatsalan2017privacy}.

For example, during the COVID-19 pandemic, the inability to efficiently link data between testing centres, hospitals and vaccination sites limited timely tracking of infection rates and vaccination outcomes.
Had more robust data linkage mechanisms been in place, public health officials could have responded more effectively to outbreaks and targeted interventions to specific populations.
Data linkage thus plays a critical role in transforming fragmented data landscapes into unified and actionable insights, leading to more informed decision-making.
In response to the COVID-19 pandemic, organisations such as the Centers for Disease Control and Prevention and the Food and Drug Administration have launched projects to address these challenges and further develop linkage techniques \cite{pathak2024privacy}.

In scenarios such as the COVID-19 pandemic, data integration efforts often involve linking records on natural persons from multiple sources.
For example, integrating data from different healthcare providers, laboratories and public health agencies typically requires the use of pseudo-identifiers derived from \ac{pii}, such as names, dates of birth or other sensitive information.
However, reliance on \ac{pii} for linkage raises significant privacy concerns, as improper handling of such data can lead to re-identification of individuals, with potentially serious consequences such as data breaches, identity theft or unauthorised access to personal health information \cite{pathak2024privacy, schnell2009privacy}.

The increasing digitisation of personal data has already led to large-scale data breaches, demonstrating the risks of improperly secured data.
Notable incidents such as the Cambridge Analytica scandal, in which personal data was misused for political profiling, highlight the ethical and regulatory challenges of data integration \cite{isaak2018user}.
Similarly, healthcare data leaks have raised concerns about the implications of unauthorised access to medical histories, genetic data and insurance records.
Leaks of personal health information can have serious consequences, including blackmail, discrimination, and fraud, which can cause significant personal harm.
For example, individuals whose medical histories are exposed may face discrimination in employment or insurance, while others may become targets of scams that exploit their health conditions.
The potential for such abuse underlines the critical importance of robust data protection measures by working with \ac{pii} \cite{smith2016examining}.

To address these privacy risks, various techniques have been developed to protect \ac{pii} during the linking process, primarily by encoding the data prior to linking.
However, the use of encoded \ac{pii} as pseudo-identifiers presents additional challenges.
The key question is how to efficiently decode sensitive information while maintaining the ability to accurately match records \cite{schnell2009privacy}.

Therefore, \ac{pprl} techniques are designed to facilitate data integration without exposing sensitive information, ensuring that datasets can be securely linked across different entities.
To enable linkage while preserving privacy, similarity preserving encoding is applied to the \ac{pii}.
Without such similarity preserving encoding, matches between encoded entities in different databases would not be possible \cite{schnell2009privacy, vatsalan2017privacy}.
Over time, three main privacy-preserving encoding schemes have emerged as enablers for \ac{pprl} \cite{vidanage2020graph, schaefer2024}.

\ac{bf} encoding is the most widely used technique in \ac{pprl} and is often considered the reference standard \cite{schaefer2024}.
Originally introduced by Burton Bloom in 1970 as a probabilistic data structure for efficient set membership testing \cite{bloom1970space}, \ac{bf}s were later adapted for \ac{pprl} due to their simplicity and efficiency in both storing and computing set similarities.
Their compact representation and probabilistic nature make them ideal for scalable \ac{pprl} systems, especially in environments dealing with large datasets \cite{schnell2009privacy}.
The seminal work of Schnell et al. \cite{schnell2009privacy} demonstrated the use of \ac{bf}s in \ac{pprl}, particularly in healthcare, highlighting their ability to perform secure record matching without exposing sensitive identifiers \cite{schnell2009privacy}.

However, \ac{bf}s are not without limitations.
Their vulnerability to graph-based attacks and pattern exploitation has driven research into improving their security.
Techniques such as diffusion have been proposed to obscure recognisable patterns and increase security \cite{schaefer2024,armknecht2023strengthening}.
For example, Armknecht et al. \cite{armknecht2023strengthening} explored methods to strengthen the security of \ac{bf} by adding a linear diffusion layer to the \ac{bf}-based \ac{pprl} approach, which complicates pattern mining attacks.

To address some of these weaknesses of \ac{bf}s, \ac{tmh} encoding has been introduced as a more secure alternative.
MinHash, first developed by Broder in 1997 for estimating set similarities in large document collections \cite{broder1997resemblance}, has been adapted using tabulation-based hashing \cite{smith2017secure}.
Although less widely used than \ac{bf}s, \ac{tmh} offers distinct advantages, including stronger security guarantees against re-identification attacks.
However, these benefits come at the cost of increased computational complexity and memory usage, which may limit its applicability in resource-constrained environments \cite{smith2017secure}.

A further development in encoding techniques is the introduction of \ac{tsh} encoding, which aims to combine the strengths of both \ac{bf}s and \ac{tmh} while mitigating their respective weaknesses.
As detailed by \cite{ranbaduge2020secure}, \ac{tsh} employs a two-stage process: data is first encoded using multiple \ac{bf}s, followed by an additional hashing layer that transforms the encoded data into a set of integers suitable for similarity comparison.
This layered approach enhances privacy by adding an extra layer of obfuscation, making it more resistant to attack, while maintaining efficient similarity computations \cite{vidanage2020graph, ranbaduge2020secure}.

In practice, \ac{bf}-based \ac{pprl} has become the dominant standard and is widely used in areas such as crime detection, fraud prevention and national security due to its balance of efficiency and ease of implementation.
However, \ac{bf}-based \ac{pprl} systems are not without limitations and vulnerabilities.
Previous research has shown that there are several attacks targeting \ac{pprl} systems, with a focus on exploiting the weaknesses inherent in \ac{bf} encodings.
These attacks specifically target weaknesses in \ac{bf} constructions, such as the weaknesses introduced by possible double hashing, structural flaws in filter design, and susceptibility to common pattern-mining techniques.
Notably, no specific attacks have been developed for \ac{tmh} or \ac{tsh} encodings, suggesting that research has focused primarily on the more widely used \ac{bf} scheme \cite{vidanage2020graph}.

However, a more recent and practical attack has emerged that exploits vulnerabilities common to all \ac{pprl} encoding schemes.
The \ac{gma} uses publicly available data, such as telephone directories, to re-identify encoded individuals based on overlapping records between plaintext and encoded databases \cite{vidanage2020graph, schaefer2024}.
Unlike previous attacks that focus solely on the encoding scheme of \ac{bf}s, the \ac{gma} works independently of the encoding scheme chosen.
It therefore exploits the graph structure of encoded datasets to re-identify records. Given two datasets - a plaintext reference dataset and an encoded dataset - an attacker can construct similarity graphs where nodes represent individuals and edges represent similarity scores.
By solving a graph isomorphism problem, attackers can infer one-to-one mappings between encoded and plaintext records, effectively breaking the privacy guarantees of \ac{pprl}.
The effectiveness of \ac{gma}s depends on the overlap between the two sets of data; the greater the overlap, the higher the probability of successful re-identification.
While \ac{gma}s can successfully re-identify individuals present in both the plaintext and encoded datasets, their effectiveness is limited to the overlapping subset of the two databases \cite{schaefer2024,vidanage2020graph}.

This work aims to go beyond traditional \ac{gma}s by re-identifying not only individuals present in the overlapping datasets, but as many individuals as possible from the encoded \ac{pprl} data.
To achieve this, the newly introduced \ac{dea} builds on the foundations laid by \ac{gma}s.
The \ac{dea} uses an \ac{ann} trained on the subset of previously re-identified individuals to predict and decode the remaining encoded records.
In doing so, the \ac{dea} significantly expands the scope and effectiveness of the attack, enabling broader de-anonymisation of \ac{pprl} datasets beyond the limitations of existing graph-based methods.

\section{Motivation} \label{sec:motivation}

The increasing use of \ac{pprl} in highly sensitive areas such as healthcare, finance and national security requires research to validate existing techniques and ensure robust privacy \cite{schnell2009privacy}.
As data-driven applications continue to evolve, the complexity and volume of data being collected and linked across multiple sources is growing rapidly.
While \ac{pprl} systems are designed to facilitate secure data integration without compromising privacy, evolving cybersecurity threats and attack techniques highlight the urgent need to reassess the resilience of these systems \cite{vatsalan2017privacy}.

Privacy has always been a critical concern in data management, but its importance has been intensified in the era of \ac{ai} and \ac{ml}.
These technologies increasingly rely on large data sets, often containing sensitive \ac{pii} such as medical records, financial transactions or behavioural data for training. If compromised, the exposure of such data can lead to serious privacy violations, including identity theft, financial fraud and discrimination.
The rise of data brokerage, where personal information is collected, aggregated and sold - often without explicit user consent - further exacerbates privacy concerns.
This commoditisation of personal data has made \ac{pii} an attractive target for malicious actors, increasing the risk of unauthorised data linking and re-identification attacks.
As \ac{ai} models become more advanced, the demand for rich, high-quality data continues to grow, making privacy an increasingly pressing issue \cite{king2024rethinking, manheim2019artificial}.

In this context, the vulnerability of \ac{pprl} systems to emerging attack methods is of particular concern.
While \ac{pprl} techniques such as \ac{bf} are designed to hide sensitive identifiers during the data linkage process, recent research has shown that these systems are vulnerable to \ac{gma}s.
\ac{gma}s exploit the similarity preserving properties of common encoding schemes to re-identify individuals by comparing patterns in encoded records with those in publicly available plaintext records.
This approach undermines the fundamental goal of \ac{pprl}: to protect sensitive data during the record linkage process.
Although current \ac{gma}s are limited to re-identifying individuals present in both the encoded and plaintext datasets, even partial data exposure in highly sensitive areas can have serious consequences \cite{schaefer2024,vidanage2020graph}.

The introduction of \ac{dea}s poses an even greater threat to the integrity of \ac{pprl} systems.
Unlike \ac{gma}s, \ac{dea}s aim to extend the scope of re-identification to as many individuals as possible within the encoded database.
Using \ac{ann}s trained on previously decoded data from \ac{gma}s, \ac{dea}s can predict and decode additional records, potentially leading to the complete de-anonymisation of entire encoded datasets.
This represents a paradigm shift, as it challenges the viability of widely used \ac{pprl} techniques, such as \ac{bf}-based encoding, which have been considered secure.

The primary motivation for this research is to proactively investigate and demonstrate the consequences of such advanced attacks in order to prevent their realisation in real-world scenarios.
By exposing the potential vulnerabilities of \ac{pprl} systems, this work aims to demonstrate how attackers could exploit decrypted data to compromise privacy on a large scale.
A successful implementation of the \ac{dea} will provide empirical evidence that state-of-the-art methods are insufficiently secure, highlighting the urgent need for more robust privacy-preserving techniques.

Furthermore, there is a notable gap in current research regarding the extension of attack capabilities beyond the intersection of datasets.
While significant efforts have been made to address the vulnerabilities exposed by \ac{gma}s, there is a lack of comprehensive studies exploring how \ac{ml} can be used to generalise these attacks and compromise entire databases.
This research aims to fill this gap by developing and evaluating the \ac{dea}, thereby contributing to a broader understanding of \ac{pprl} vulnerabilities.

By addressing this gap, this thesis aims to contribute to the body of knowledge on \ac{pprl} vulnerabilities and serve as a foundation for future research aimed at strengthening these systems.
The knowledge gained from this study will not only enable the development of more secure \ac{pprl} techniques, but will also influence best practices in privacy and security.

\section{Related Work}  \label{sec:rel-work}

The study by Vidanage et al. \cite{vidanage2020graph} represents a significant advance in the field of \ac{pprl} through the introduction of a new attack method known as \ac{gma}.
Their work begins with a comprehensive overview of \ac{pprl} systems and the similarity preserving encoding techniques commonly used, such as \ac{bf}s.
The \ac{gma} exploits weaknesses in these encoding schemes by exploiting their ability to preserve partial similarity information even after encoding.
By constructing similarity graphs from both encoded and plaintext datasets, the \ac{gma} solves a graph isomorphism problem to align nodes and successfully re-identify individuals in the encoded dataset using publicly available sources such as telephone directories.
This method demonstrates the universal applicability of \ac{gma}s across different \ac{pprl} schemes, and highlights a critical weakness in systems previously thought to be more robust \cite{vidanage2020graph}.

Building on this foundation, Schäfer et al. \cite{schaefer2024} revisited and extended the work of Vidanage et al.
Their contribution lies in a meticulous reproduction and replication of the original \ac{gma}, during which they identified a critical flaw: an undocumented pre-processing step in the provided codebase that inadvertently increased the effectiveness of the attack.
While this step was originally intended to improve computational performance, it introduced errors into the proposed \ac{gma}.
Schäfer et al. corrected this problem and further optimised the \ac{gma}, resulting in improved robustness and efficiency.
Their improved implementation achieved higher re-identification rates compared to the original approach.
This improvement not only validates the vulnerabilities highlighted by the \ac{gma}, but also highlights the potential for refining attack methodologies to expose even greater weaknesses in \ac{pprl} systems.

The work of Schäfer et al. \cite{schaefer2024} is particularly relevant to this thesis, as their improved \ac{gma} implementation and accompanying codebase form the basis of the \ac{dea} proposed in this study.
While the \ac{gma} is limited to re-identifying individuals present in both encoded and plaintext datasets, the \ac{dea} seeks to extend the scope of re-identification beyond this intersection.
Using \ac{ann}s trained on the re-identified individuals from the \ac{gma}, the \ac{dea} aims to predict and decode additional datasets, potentially leading to complete de-anonymisation of encoded datasets.

To date, no existing research has proposed an approach comparable to the \ac{dea}.
This thesis addresses this gap by developing and evaluating the \ac{dea}, thereby contributing to a broader understanding of \ac{pprl} vulnerabilities and highlighting the urgent need for more secure data linking techniques.

\section{Contribution}  \label{sec:contribution}

The contribution of this thesis is divided into three main parts.
First, a comprehensive analysis of \ac{pprl} systems is carried out, with particular emphasis on the three main encoding schemes: \ac{bf} encoding, \ac{tsh} encoding and \ac{tmh} encoding.
This analysis aims to highlight the basic principles, strengths and weaknesses of each encoding scheme, setting the stage for the subsequent investigation of their susceptibility to \ac{dea}.

Next, the current state of the art \ac{gma} is analysed and its limitations are discussed in detail.
Although \ac{gma}s have proven effective in re-identifying individuals within overlapping datasets, their applicability is limited to the intersection of plaintext and encoded records.
This inherent limitation highlights the need for more advanced attack strategies that can go beyond this.

The main focus of this thesis is the implementation and evaluation of the \ac{dea}, which attempts to outperform the capabilities of \ac{gma}s by decrypting a larger fraction of encoded records.
To achieve this, the thesis examines the conceptual foundations, theoretical underpinnings and technical requirements of the \ac{dea}.
Building on the initial re-identifications made by the \ac{gma}, the \ac{dea} employs a supervised machine learning approach, specifically using \ac{ann}s trained on previously decoded data to predict and re-identify remaining encoded records.
This method significantly extends the scope of de-anonymisation in \ac{pprl} systems and provides a novel approach to current research.

The \ac{dea} is then evaluated against the three main \ac{pprl} encoding schemes.
While the specific encoding scheme has minimal impact on the \ac{gma}, which is primarily based on solving a graph isomorphism problem, it plays a role in the \ac{dea}.
This is due to the fact that the \ac{ann} has to be trained separately for each encoding scheme to account for the unique structural features and nuances of the encoding.
However, the \ac{dea} is designed with adaptability in mind, ensuring that it can be effectively applied across different encoding schemes, thus increasing its generalisability and practical relevance.

Through this research, the thesis aims to answer critical questions about the robustness of \ac{pprl} systems.
It investigates how effective supervised machine learning-based \ac{dea}s are at re-identifying the remaining entries that \ac{gma}s cannot decode.
It also examines how different encoding schemes affect the performance and accuracy of the \ac{dea}, providing insight into which schemes are more susceptible to such attacks and why.
By addressing these issues, the thesis contributes to a deeper understanding of the vulnerabilities inherent in \ac{pprl} systems and lays the groundwork for the development of more secure privacy preserving techniques.

\section{Organization of this Thesis}  \label{sec:orga}

This thesis is divided into four main sections: technical background, methodology, results, and conclusion.

First, an overview of \ac{pprl} systems is given, with particular emphasis on a thorough analysis of the most commonly used encoding techniques.
Next, the existing \ac{gma} is introduced and explained in order to provide the basis for the study.
In addition, an overview of \ac{ann}s is given to provide the necessary background knowledge.

Next, a detailed description of the attack model for the \ac{dea} is outlined, including how \ac{ann}s are used to enhance the attack.
This is followed by an explanation of the actual implementation of the \ac{dea}, along with a discussion of the experiments conducted.
The results of the \ac{dea} on different encoding schemes are then analysed and evaluated.
Finally, the thesis concludes with a summary of the main contributions, a discussion of the broader implications, and suggestions for future research.