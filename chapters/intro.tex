\chapter{Introduction}  \label{sec:introduction}

Data and record linkage is a critical component of both research and software development, facilitating the integration of data from diverse sources to generate richer, more comprehensive insights about the same entity. 
Initially, deterministic record linkageâ€”which relies on exact matches across predefined identifiers like unique id's was the predominant method used in early database systems. 
However, deterministic approaches often fail in real-world scenarios where data may suffer from inconsistent formatting, typographical errors, or missing values, making exact matches impossible.

The introduction of a probabilistic record linkage framework by Fellegi et al. \cite{fellegi1969theory} 1969 marked a significant advancement in overcoming these limitations. 
Their work, \enquote{A Theory for Record Linkage} \cite{fellegi1969theory}, proposed a statistical method for linking records across datasets by calculating the probability that two records refer to the same entity, even when discrepancies are present in the data. 
This approach evaluates common attributes assigning weights based on the likelihood of a match versus a coincidental similarity. 
By accounting for the inherent imperfections in real-world data, the so called Fellegi-Sunter model became a foundational methodology for data integration, particularly in heterogeneous and distributed data environments where traditional deterministic methods fall short.

Such a record linkage approach is particularly important in sectors such as healthcare and the social sciences, where data is often distributed across multiple institutions and sources. 
In these fields, the ability to integrate disparate datasets is essential for gaining comprehensive insights, improving outcomes, and informing policy decisions.
For example, in the United States, the healthcare system is highly fragmented, consisting of numerous independent entities such as hospitals, clinics, insurance providers, public health agencies, and research institutions. 
Each of these organizations collects and stores patient data independently, often using different systems and formats. 
This fragmentation creates significant challenges when attempting to track patient outcomes, monitor disease outbreaks, or evaluate the effectiveness of treatments across the entire population.
Effective data linkage can bridge these gaps by connecting records that refer to the same individual across multiple datasets. In healthcare, this means a patient's hospital records, insurance claims, laboratory test results, and public health data can be combined to provide a more complete picture of their health status and treatment history. 
This integration is critical for tasks such as epidemiological research, public health surveillance, personalized medicine, and healthcare quality improvement.

For instance, during the COVID-19 pandemic, the inability to efficiently link data between testing centers, hospitals, and vaccination sites hindered the timely tracking of infection rates and vaccination outcomes. 
Had more robust data linkage mechanisms been in place, public health officials could have responded more effectively to outbreaks and tailored interventions to specific populations. 
Thus, record linkage plays a pivotal role in transforming fragmented data landscapes into unified, actionable insights, ultimately leading to better health outcomes and more informed decision-making.
In response, organisations such as the Centers for Disease Control and Prevention and the Food and Drug Administration have launched projects to address these challenges and further develop linkage techniques \cite{pathak2024privacy}.

In scenarios such as the COVID-19 pandemic, data integration efforts often involve linking records on natural persons from multiple sources.
For example, integrating data from different healthcare providers, laboratories and public health agencies typically requires the use of pseudo-identifiers derived from \ac{pii} such as names, dates of birth or other sensitive information. 
However, reliance on \ac{pii} raises significant privacy concerns, as improper handling of such data can lead to re-identification of individuals, with potentially serious consequences such as data breaches, identity theft or unauthorised access to personal health information \cite{pathak2024privacy}.

The increasing digitization of personal data has already led to large-scale privacy breaches, demonstrating the risks of improperly secured data. 
Notable incidents, such as the Cambridge Analytica scandal, where personal data was misused for political profiling, highlight the ethical and regulatory challenges in data integration \cite{isaak2018user}. 
Similarly, healthcare data leaks have raised concerns about the implications of unauthorized access to medical histories, genetic data, and insurance records.
Leaks of personal healthcare data can lead to severe consequences, including blackmail, discrimination, and scams, which can cause significant personal harm. 
For instance, individuals whose medical histories are exposed may face discrimination in employment or insurance, while others may become targets for scams exploiting their health conditions. 
The potential for such misuse underscores the critical importance of robust data protection measures by working with \ac{pii}.

To address these privacy risks, various techniques have been developed to protect \ac{pii} during the linking process, primarily by encrypting the data prior to linking. 
However, the use of encrypted \ac{pii} as pseudo-identifiers presents additional challenges.
The key question is how to efficiently encrypt sensitive information while maintaining the ability to accurately match records. 
In addition, performing linkage on encrypted data requires specialised procedures that work effectively without compromising the privacy of the underlying information.

Therefore, \ac{pprl} techniques are designed to facilitate data integration without exposing sensitive information, ensuring that datasets can be securely linked across different entities.
In order to still be able to perform linkage while preserving privacy, similarity preserving encodings are applied to the \ac{pii}.
Without such similarity preserving encryption, matches between encrypted entities in different databases would not be possible.

One of the core challenges in \ac{pprl} is therefore balancing data privacy with usability. 
While strong encryption techniques provide excellent protection, they make record linkage infeasible due to the loss of structural similarity. 
On the other hand, similarity-preserving encodings, enable matching while still attempting to obscure exact values. 
This trade-off between privacy and utility is a key aspect, as stronger privacy mechanisms often degrade the quality of linkage, making it difficult to achieve high matching accuracy.

Over time, three main privacy-preserving encoding schemes have emerged as enablers for \ac{pprl}. \cite{vidanage2020graph, schaefer2024}

\ac{bf} encoding is the most widely used technique in \ac{pprl} and is often considered the reference standard. 
Originally introduced by Burton Bloom in 1970 as a probabilistic data structure for efficient set membership testing \cite{bloom1970space}, Bloom Filters (BFs) were later adapted for PPRL due to their simplicity and efficiency in both storing and computing set similarities. 
Their compact representation and probabilistic nature make them ideal for scalable \ac{pprl} systems, particularly in environments dealing with large datasets. 
The seminal work by Schnell et al. demonstrated the application of Bloom Filters in privacy-preserving record linkage, particularly within healthcare settings, highlighting their ability to perform secure record matching without exposing sensitive identifiers \cite{schnell2009privacy}.
However, BFs are not without limitations. 
Their vulnerability to frequency-based attacks and pattern exploitation has driven research into enhancing their security. 
Techniques such as diffusion layers have been proposed to obscure recognizable patterns and reduce susceptibility to attacks like graph-based re-identification. 
For instance, Armknecht et al. explored methods to reinforce the security of Bloom Filters by introducing additional randomization, which complicates adversarial analysis and pattern mining attacks.

To address some of these vulnerabilities,\ac{tmh} has been introduced as a more secure alternative. 
\ac{tmh}, first developed by Broder (1997) for estimating set similarities in large-scale document collections \cite{broder1997resemblance}, was adapted using tabulation hashing to enhance its randomness and reduce collision rates in PPRL applications. 
Although less commonly used than BFs, TMH offers distinct advantages, including stronger security guarantees against re-identification attacks. 
However, these benefits come at the cost of increased computational complexity and greater memory usage, which may limit its applicability in resource-constrained environments.

A further advancement in encoding techniques is the introduction of \ac{tsh}, which aims to combine the strengths of both \ac{bf}s and \ac{tmh} while mitigating their respective weaknesses.
As detailed by Vatsalan and Christen (2016), \ac{tsh} employs a two-phase process: data is first encoded using multiple \ac{bf}s, followed by an additional hashing layer that transforms the encoded data into a set of integers suitable for similarity comparisons \cite{vatsalan2016privacy}. 
This layered approach enhances privacy by adding an extra layer of obfuscation, making it more resistant to attacks while maintaining efficient similarity computations.

In practice, \ac{bf}-based \ac{pprl} has become the dominant standard and is widely used in areas such as crime detection, fraud prevention, and national security due to its balance of efficiency and ease of implementation.
However, \ac{pprl} systems are not without limitations and vulnerabilities. 
Previous research has shown that there are several attacks targeting \ac{pprl} systems, with a focus on exploiting the weaknesses inherent in \ac{bf} encodings.
These attacks specifically target vulnerabilities in \ac{bf} constructions, such as the weaknesses introduced by double hashing, structural flaws in the filter design, and susceptibility to frequent pattern-mining techniques. 
In addition, language model-based attacks and graph-based dictionary attacks have been used to compromise \ac{bf}-encoded data. 
Notably, no specific attacks have been developed for \ac{tmh} or \ac{tsh} encodings, suggesting that research has focused primarily on the more widely used \ac{bf} scheme. \cite{vidanage2020graph}


However, a more recent and sophisticated attack has emerged that exploits vulnerabilities common to all \ac{pprl} encoding schemes. 
The \ac{gma} uses publicly available data, such as telephone books, to re-identify encrypted individuals based on overlapping records between plaintext and encrypted databases \cite{vidanage2020graph, schaefer2024}. 
Unlike previous attacks that focused solely on the encoding scheme of \ac{bf}s, the \ac{gma} works independently of the encoding scheme.
It therefore exploits the graph structure of encoded datasets to re-identify records. Given two datasets â€” a plaintext reference dataset and an encoded dataset â€” an attacker can construct similarity graphs where nodes represent individuals and edges represent similarity scores. 
Using graph isomorphism techniques, attackers can infer one-to-one mappings between encoded and plaintext records, effectively breaking the privacy guarantees of \ac{pprl}. 
The effectiveness of \ac{gma}s depends on the overlap between the two datasets; the larger the intersection, the higher the probability of successful re-identification. 
While \ac{gma}s can successfully re-identify individuals present in both the plaintext and encrypted datasets, their effectiveness is limited to the overlapping subset of the two databases \cite{schaefer2024}.

This work aims to go beyond traditional \ac{gma}s by re-identifying not only individuals present in the overlapping datasets, but as many individuals as possible from the encrypted \ac{pprl} data. 
To achieve this, the newly introduced \ac{dea} builds on the foundations laid by \ac{gma}s. 
The \ac{dea} uses a neural network trained on the subset of previously re-identified individuals to predict and decode the remaining encrypted records. 
In doing so, the \ac{dea} significantly expands the scope and effectiveness of the attack, enabling broader de-anonymisation of \ac{pprl} datasets beyond the limitations of existing graph-based methods.


\section{Motivation}  \label{sec:motivation}
The increasing use of \ac{pprl} in highly sensitive areas such as healthcare, finance and national security requires rigorous research to validate existing techniques and ensure robust data protection. 
As data-driven use cases that require approaches like \ac{pprl} continue to evolve, so do the methods used by malicious actors to exploit these systems.
While privacy has always been a critical concern, its importance has grown in an era dominated by \ac{ai} and \ac{ml}. 
These technologies increasingly rely on large data sets, many of which contain \ac{pii} that, if compromised, could lead to significant privacy violations. 
In addition, the rise of data brokerage, where personal data is collected, aggregated and sold - often without explicit user consent - has increased concerns about data misuse. 
As \ac{ai} models become more advanced, the demand for rich, high-quality data increases, making privacy a pressing issue \cite{ldc2024,cacgroup2024,arxiv2024}.

In this context, the vulnerability of \ac{pprl} systems to emerging attack modes is of particular concern. 
As highlighted in the introduction, researchers have shown that \ac{pprl} systems are vulnerable to \ac{gma}s, which exploit the similarity preserving properties of common encoding schemes to re-identify individuals. 
These attacks directly undermine the primary objective of \ac{pprl}: to protect sensitive data during the linking process. 
Although current \ac{gma}s are limited to re-identifying individuals present in both encrypted and plaintext datasets, this still represents a significant privacy risk, especially in domains where even partial data exposure can have serious implications.

The potential implementation of a \ac{dea} poses even greater risks. 
Unlike \ac{gma}s, which are limited to the intersection of records, \ac{dea}s aim to extend the attack to re-identify as many people as possible from the encrypted database. 
Using neural networks trained on previously decoded data, \ac{dea}s can predict and decode additional records, potentially leading to the complete de-anonymisation of entire encrypted datasets. 
This not only undermines the privacy guarantees offered by \ac{pprl} systems, but also raises critical questions about the future viability of widely used techniques such as \ac{bf}-based \ac{pprl}.

The primary motivation for this research is to proactively investigate and demonstrate the consequences of such advanced attacks in order to prevent their realisation in real-world scenarios.
By exposing the potential vulnerabilities of \ac{pprl} systems, this work aims to show how attackers could exploit decrypted data to compromise privacy on a large scale. 
A successful implementation of the \ac{dea} will show that state-of-the-art methods are insufficiently secure, emphasising the urgent need for more robust privacy-preserving techniques.
Furthermore, the lack of comprehensive research into extending the scope of attacks beyond the intersection of datasets underscores the need for this study. 
While significant efforts have been made to address the vulnerabilities exposed by \ac{gma}s, there is a notable gap in understanding how these systems can be compromised on a wider scale.

By addressing this gap, this thesis aims to contribute to the body of knowledge on \ac{pprl} vulnerabilities and serve as a foundation for future research aimed at fortifying these systems against increasingly advanced threats.



\section{Related Work}  \label{sec:rel-work}
% Provide a literature review on existing PPRL literatur, encoding methods like Bloom filters, MinHash, and two-step hashing
% Discuss previous research on GMAs and their limitations
% Highlight the gap in current research regarding decoding complete databases.

The work of Vidanage et al. \cite{vidanage2020graph} introduces a novel attack against \ac{pprl}, known as the \ac{gma}.
The authors begin with an overview of \ac{pprl}, encoding methods, and present attacks on \ac{pprl} systems to date.
Their novel attack exploits the similarity preserving properties of commonly used coding schemes, such as \ac{bf}, making it universally applicable to various \ac{pprl} schemes.
Using a graph-based approach and solving a graph isomorphism problem, the \ac{gma} aligns nodes in similarity graphs to successfully re-identify individuals using a publicly available database \cite{vidanage2020graph}.
This work is fundamental to this thesis, as the \ac{dea} builds on the re-identified individuals produced by the \ac{gma} and extends the attack.

Another important contribution comes from SchÃ¤fer et al. \cite{schaefer2024}, who revisit and extend the work of Vidanage et al. \cite{vidanage2020graph}.
The authors provide a thorough reproduction and replication of the proposed \ac{gma}, uncovering an undocumented pre-processing step in the original codebase that unintentionally affected the success rate of the attack.
This undocumented pre-processing step was intended to improve performance, but instead introduced implementation errors.
SchÃ¤fer et al. addressed this problem by correcting the pre-processing and enhancing the \ac{gma} to improve both robustness and efficiency.
Their improved implementation achieved higher re-identification rates than the original approach of Vidanage et al. \cite{schaefer2024}.
The work of SchÃ¤fer et al. is particularly relevant to this thesis, as their improved \ac{gma} implementation and corresponding code base serve as the basis for the \ac{dea} proposed in this thesis.
Therefore, small tweaks will be made to the \ac{gma} implementation to build on the results of the attack.

Currently there exists no approach like the proposed \ac{dea} in research. Therefore, this thesis aims to fill this gap.


%Bloom Filters

%MinHash

%Two Step Encoding

%GMA by Jochen and Vidanage

%Gap for decoding complete database


\section{Contribution}  \label{sec:contribution}
%Summarize the key contributions of your thesis:
% - Comprehensive analysis of the vulnerabilities in PPRL systems using GMA's
% - Enhancement of the existing attack approaches by introducing Dataset Extension Attacks using Neural Networks
% - Development and application of a neural network-based DEA using the provided codebase
% - Evaluation of the impact of different encoding schemes on the effectiveness of DEAs
% - Provide insights into the broader implications of these attacks on data privacy

The contribution of this thesis is divided into three main parts. 
First, a comprehensive analysis of \ac{pprl} systems is carried out, with particular emphasis on the three major encoding schemes: \ac{bf} encoding, \ac{tsh} encoding, and \ac{tmh} encoding. 
This analysis aims to highlight the basic principles, strengths and weaknesses of each encryption scheme, setting the stage for the subsequent investigation of their susceptibility to the \ac{dea}.

Next, the current state of the art \ac{gma} is analysed and its limitations are discussed in detail. 
Although \ac{gma}s have proven effective in re-identifying individuals within overlapping datasets, their applicability is limited to the intersection of plaintext and encrypted records. 
This inherent limitation highlights the need for more advanced attack strategies that can go beyond this.

The main focus of this thesis is the implementation and evaluation of \ac{dea}, which attempts to surpass the capabilities of \ac{gma}s by decrypting a larger fraction of encrypted records. 
To achieve this, the thesis examines the conceptual foundations, theoretical underpinnings and technical requirements of the \ac{dea}. 
Building on the initial re-identifications made by the \ac{gma}, the \ac{dea} employs a supervised machine learning approach, specifically using neural networks trained on previously decoded data to predict and re-identify remaining encrypted records. 
This method significantly extends the scope of de-anonymisation in \ac{pprl} systems and provides a novel approach to current research.

The \ac{dea} is then evaluated against the three major \ac{pprl} encoding schemes. 
While the specific encoding method has minimal impact on the \ac{gma}, which relies primarily on solving graph isomorphism problems, it plays a pivotal role in the \ac{dea}. 
This is due to the fact that the neural network must be trained separately for each encoding scheme to account for the unique structural characteristics and encoding nuances. 
However, the \ac{dea} is designed with adaptability in mind, ensuring that it can be effectively applied across different coding schemes, thus increasing its generalisability and practical relevance.

Through this research, the thesis aims to answer critical questions about the robustness of \ac{pprl} systems. 
It investigates how effective supervised machine learning-based \ac{dea}s are at re-identifying the remaining entries that \ac{gma}s cannot uncover.
It also examines how different encoding schemes affect the performance and accuracy of the \ac{dea}, providing insight into which schemes are more susceptible to such attacks and why. 
By addressing these questions, the thesis contributes to a deeper understanding of the vulnerabilities inherent in \ac{pprl} systems and lays the groundwork for the development of more secure privacy-preserving techniques.




\section{Organization of this Thesis}  \label{sec:orga}
% Outline the structure of the tesis:
% - provide an overview of PPRL systems and common encoding techniques
% - review existing work on GMAs, setting the stage for my contribution
% - detail the methodology used in my study, including the neural network implementation
% - Do the actual work and implement something
% - present the results of the DEA across different encoding schemes and discusses the findings
% - conclude the thesis with a summary of contributions, implications, and suggestions for future research

This thesis is divided into X main sections.
First, an overview of \ac{pprl} systems is given, with particular emphasis on a thorough analysis of the most commonly used encoding techniques.
Next, the existing \ac{gma} is introduced and explained in order to provide the basis for the study.
In addition, an overview of neural networks is given to provide the necessary background knowledge.

Next, a detailed description of the attack model for the \ac{dea} is outlined, including how neural networks are used to enhance the attack.
This is followed by an explanation of the actual implementation of the \ac{dea}, along with a discussion of the experiments conducted.
The results of the \ac{dea} on different encryption schemes are then analysed and evaluated.
Finally, the thesis concludes with a summary of the main contributions, a discussion of the broader implications, and suggestions for future research.