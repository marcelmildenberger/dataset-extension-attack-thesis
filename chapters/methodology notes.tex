\chapter{Methodology}  \label{sec:method}
%Describe Goal of this chapter
Describe and present the novel DEA
Give an overview about the modifications done to the gma,and the design and implementation of the dea

%Describe How DEA build upon GMA
By extending
Therefore results of the GMA are needed
Results are needed in a certain format to enable the DEA and neural network training


%Main steps and decisions taken
1. Extract results of GMA in a pre-formatted way
2. Setup Datasets which transforms data to tensors to work with neural networks
3. Define Test, Validation and training set and set up respective data loaders
4. Define neural network with its corresponding architecture of inputlayer, hidden layers and output layers
5. Define loss function, optimizer
6. Train model based on training set and use validation set to prevent overfitting
7. Apply model to not-reidentified indivuals => output is multi-label classification witha  probability for each possible 2-gram that it occours
8. Filter out relevant 2-grams (using a threshhold to remove unrelevant)
9. Reconstruct PII out from predicted 2-grams

\section{Modifications to the \ac{gma}} \label{sec:modifications}
%How GMA serves as basis
Serves as important basis
It provides the re-identified individuals with their corresponding encodings
Performance of GMA influences performance of DEA enourmously
Low GMA re-identification rate => less training data => less re-identification potential


%Which parts are reused and which need modification
GMA from Schaefer et al. reused
Small modifications done => GMA initially just outputted a mapping of the ids for re-identified individuals. Therefore changes where done, which outputs data of the gma in the following format:
For reidentified individuaks: <PII> <encoding> <uid>
for not-reidentified individuals: <encoding> <uid>
note that uid is only included for research and testing purposed for the developer to be able to re-identify individuals manually. In a real world attack they can be neglected and are not needed/inclduded in any of the training/usage of the DEA or neural network

Some parts of the GMA were deleted as not required (e.g. encoding schemes except bf, tmh, tsh) to make it more lean

%Describe therefore the starting point of the DEA
- Reidentified individuals in the format as mentioned above
- Not-reidentified individuals in the format as mentioned above

\section{Design and Implementation of the \ac{dea}} \label{sec:designandimplementation}

This section is going to Descrie the problem definiton, data representation, architecture and training of the Model
focus on the difference encoding schemes
Explaining the main approach which is then modified for each encoding scheme
This means: every encoding schemes has its own dataset structure (differen inputs for the neural network) but same output (2-grams) and its own neural network (different encoding schemes, inputs requires different architectures)

\subsection{Problem Definition} \label{sec:problemdefinition}
%Main challenge: extending re-identifications beyond
GMA is limited
Potential to further re-identify indivduals exists
Extending re-identification could be done by inclduing more publicly available data and try rerunning dea again
Or:
%Reconstructing deterministic relationship between used encodings scheme (which are using hash functions) and plaintext
Trying to reconstruct the deterministic relationship between the used encoding and the corresponding 2-grams of the PII
This is possible because in all encoding schemes hash functions are used which are mapping an arbitrary length input to a fixed output.
Hash functions are deterministic, this property is trying to be exploited by the linkage unit training neural networks
Problems are that number and which hash functions are used is unknown, but this shouldnt really matter as the dea is some kind of frequency attack
What makes the DEA hard and never really working with 100\% certainty is the property of hash functions producing collisions (as arbitrary length input (infinite input space) is mapped to an fixed length output)
Collisions make it really hard for the DEA and neural network to learn the determinstics between 2-grams and the produced encoding scheme
nevertheless, it is possible to try it out and producing a probabilistic output

%Why GMA's fail to do this
GMA fails because its only exploiting the graph structure of the data

\section{Data Representation} \label{sec:representation}
%Discuss preprocessing steps required for effective neural network training

For a neural netwwork to work properly and be successfull, preprocessing the data in a consumable (i.e. tensors) format is required. this is required for the input data which will be the encodings scheme as well as for the output which will be the prediciton of the 2-grams. Therefor pytorch datasets are created which transforms the input data (re-identified individuals and not-reidentified indivuals) to the required format (i.e. tensors)

This is required for effective ANN training and performance

%Describe structure of encoded records (bf, tmh, tsh) and their representations
General data structure as mentioned before

BF encodings are a fixed length binary string based on the chosen parameters by alice
The BF bitstrings are then transformed to a pytorch tensor of the same length as the BF
This Tensors has as well the 1 bits set at the same index correspondlingy to the BF

TMH is as well as the BF a binary bitstring with a fixed length chosen by the paramets from alice
THe TMH bitrings are then transformed into a pytorch tensore of the same length as the TMH bitstring
This tensors has as well the 1 bits set at the same index correspondingly to the TMH bitsring

Fro TSH its slightly different and complicated.
TSH produces a set/list of integers with arbitrary length as encodings
But neural networks have an fixed inputlayer number of neurons and therefore excepts an input in that format
Therefore some transformation needs to be done. Aggregation is no option as it would cause loss of information which is required in this already limited-knowledge Setup
Therefore two options are exploited, one is padding. here first the largest set of all TSH encodings is looked up and the size of this set is then set as the size of the tensor. Every set is then transformed to the tensor by mapping the numbers with their respective index to to the tensor. If the set of numbers is smaller than the tensor (initated with the size of the largest set) then the remaining values are padded with zeros. E.g. if n=5 {22, 3, 4} and {11,8} are mapped to [22,3,4,0,0] and [11,8,0,0,0] respectively
The other approach (I dont have a name yet) is creating the tensors of the size equal to the largest number found. So if the largest number ever used in the TSH encoding is a 12345 then the size of all tensors will be 12345. Similar to BF and TMH the encodings of TSH are then mapped to a bitstring. Therefore numbers of the TSH are mapped in the way that the position/index corresponding to the number is set to 1. To not loose information this way (if the same number appears twice in a set), the binary string is extended in the way that its just added up (so when 22 occurs 2 times in the TSH set, the position/index 22 of the tensor would be 2 (1+1))

As the neural networks are trying to map the input of the encoding to 2-grams, the output of all neural networks will be the same: predicting the likelihood of a 2-gram being presented

%Explain how re-identified individuals serve as labeled training data
Therefore the re-identified individuals serve as a labeled training data. The PII and corresponding encoding is known. The encoding is transformed to a tensor as described above as well as the 2-grams of the PII. This is done using a dictionary of all possible 2-grams which can be constructed using ascii lower case values (aa-zz), digit 2-grams (00-99) and the corresponding overlaps (a0-z9). This is done as the PII of the datasets used consists mainly of firstnames, surnames and birthdays. This dictionary maps the position/index of a output to a corresponding 2-gram. So when the neural network outputs the probability 60\% for the index 1 this can be mapped to the likelihood of 60\% of the 2-gram ab being present.


\subsection{\ac{ann} Architecture for \ac{dea}} \label{sec:architecture}
%Justify use of neural networks to predict missing information
Trying to reconstruct encodings which are using hash functions is really hard / not possible
But neural networks  can be used as an probabilisitc appraoch to achieve this
The function of the neural networks/dea is therefore to predict 2-grams by learning on re-identified individuals
this corresponds to learning a frequency pattern (which 2-grams are mapped to which positions e.g. in a BF) by exploiting the determinstic charactersitics of hash functions


%Present architecture for each encoding scheme: Input, Hidden and output layer
BF: input layer length of BF, hidden dim ?, output layer size of the 2-grams dictionary
TMH: input layer length TMH bitstring, hidden dim?, output layer size of the 2-grams dictionary
TSH: input layer varying on chosen approach. For padding its the size of the size of the largest set. for the other approach it has the size equal to the largest integer in all sets. Output layer size of the 2-grams dictionary

%Explain choice of activation functions, loss functions and optimizer
Loss: BCEWithLogitsLoss
optimizer: Adam

\subsection{Training the Model} \label{sec:training}
%Detail dataset split (training, validation, test)
Dataloaders created for test (not-reidentified indiviuals), training (80\% from labeled dataset), validation (20\% from labeled dataset) with different batch sizes

%Discuss loss cumputation & performance metrics
Training Loss
Validation Loss
Accuracy?
Precision?
Re-identification rate using a threshhold using similarity metrics

%Explain training process and hyperparameter tuning
Epochs
for each epoch going through all training data using data loader, calculation training loss for this epoch then going through all validation data using data loader to calculate validation loss


Hyperparameter tuning?




%Where to put attacker model
%Where to put precision/accuracy and justifications
%Where to put which metric is more desired
%Where to put 2-grams => PII construction
