\documentclass[a4paper,11pt]{scrartcl}
\input{frontmatter/settings}
\input{frontmatter/definitions}
\input{frontmatter/acronyms}

\title{Summary of ``Vulnerabilities in Privacy-Preserving Record Linkage: The Threat of Dataset Extension Attacks''}
\author{Marcel Mildenberger}
\date{\today}

\begin{document}
\maketitle

\section{Motivation and Research Questions}
The increasing demand for cross-institutional data sharing in healthcare, official statistics, and scientific collaborations has made \ac{pprl} a critical enabling technology. Privacy-enhancing encodings such as \ac{bf}, \ac{tmh}, and \ac{tsh} are widely adopted because they preserve similarity relationships between records without exposing raw \ac{pii}. However, practical deployments increasingly face adversaries equipped with large auxiliary datasets and modern machine-learning toolchains. This thesis investigates how an attacker can weaponize previously leaked matches to compromise additional records that never intersected with the auxiliary data. The central research questions are threefold: (i) how far can the capabilities of the current state-of-the-art \ac{gma} be extended by subsequent statistical inference, (ii) how effectively can \ac{ann}-based models learn the structural regularities of encoded identifiers, and (iii) which encoding strategies offer tangible resistance against such dataset extension attacks under realistic conditions.

The urgency of this analysis stems from the severe societal impact of re-identifying encoded health or census data. The combination of partially overlapping datasets, large-scale public registers, and cloud-based linkage services creates new attack surfaces. While existing research has scrutinized the vulnerabilities of \ac{bf}-based linkage to direct graph alignment, little evidence existed on the feasibility of extending re-identification beyond the original overlap set. This thesis closes that gap by proposing and evaluating the \ac{dea}, a two-stage attack that uses neural models to infer plaintext character $n$-grams from encoded representations. The work goes beyond previous studies by integrating hyperparameter optimization, cross-dataset generalization, and downstream plaintext reconstruction strategies. By systematically evaluating the attack under multiple overlap scenarios and realistic ``drop-from'' strategies, the thesis provides an empirically grounded assessment of the privacy risks faced by contemporary \ac{pprl} deployments.

\section{Foundations of Privacy-Preserving Record Linkage}
\subsection{Threat Model and Similarity-Preserving Encodings}
The thesis considers a classical \ac{pprl} setting with two parties, Alice and Eve. Alice holds the encoded target database $D_e$, while Eve controls an auxiliary plaintext dataset $D_p$ that partially overlaps with $D_e$. Both parties rely on similarity-preserving encodings of string-based identifiers such as names and dates of birth. The study focuses on three prominent schemes. Bloom Filter (\ac{bf}) encoding maps overlapping character $n$-grams into a fixed-length bit array. Tabulation MinHash (\ac{tmh}) uses tabulation hashing to approximate Jaccard similarity, storing compact integer signatures. Two-Step Hashing (\ac{tsh}) first encodes data into \ac{bf}s and then applies an additional hashing layer to produce integer vectors, combining structural expressiveness with an extra diffusion step. All encodings employ Dice similarity on overlapping 2-grams, enabling direct comparison of attack outcomes across schemes.

Each encoding provides probabilistic privacy rather than information-theoretic guarantees. Prior works have shown that \ac{bf}s leak co-occurrence patterns exploitable by frequency-based or graph-alignment attacks. \ac{tmh} reduces some of these weaknesses but introduces a limited output space that still retains structural regularities. \ac{tsh} aims to combine the efficiency of \ac{bf}s with added non-linearity, yet the chained hashing can still leak correlated features once enough auxiliary matches are available. The thesis assumes a semi-honest attacker with full knowledge of the encoding parameters---a reasonable assumption because many \ac{pprl} projects publish their linkage configuration to support interoperability. Eve is further assumed to know the $n$-gram alphabet and the plaintext distribution within $D_p$.

\subsection{Baseline Capabilities and Limitations}
Before introducing the \ac{dea}, the thesis quantifies the baseline leakage that an informed adversary can achieve without machine learning. A frequency-based guesser that always predicts the globally most common 2-grams yields F1-scores around 0.23 on the \texttt{fakename} and \texttt{euro\_person} datasets, and 0.29 on the more heterogeneous \texttt{titanic\_full}. These baselines highlight that the encodings inherently leak partial information, yet they also show the ceiling of naive approaches. Crucially, the \ac{gma} alone can only recover individuals present in both $D_e$ and $D_p$, leaving the majority of Alice's database untouched. The key observation underpinning the \ac{dea} is that the re-identified subset provides labeled training data that captures the joint distribution between encoded features and plaintext $n$-grams. Leveraging this sample efficiently is the cornerstone of the proposed attack.

\section{Dataset Extension Attack Methodology}
\subsection{Attack Pipeline Overview}
The \ac{dea} orchestrates a sequence of components that transform partial re-identifications into broader privacy compromises. First, the attacker executes the improved \ac{gma} implementation by Sch√§fer et al. to align Alice's and Eve's graphs and obtain an initial set of confirmed plaintext-encoded pairs. These records serve as labeled examples for training a multi-label classifier. Each encoded record is represented as a binary or integer feature vector, depending on the underlying scheme, while the target labels correspond to the presence of specific 2-grams extracted from the plaintext. The classifier predicts the probability of each 2-gram appearing in a previously unseen encoded record. The thesis models this as a multi-label classification problem with sigmoid outputs and optimizes the Dice coefficient to balance precision and recall on sparse target vectors.

During inference, the predicted 2-gram probabilities feed three reconstruction strategies. Greedy reconstruction assembles the most likely 2-grams into candidate strings while enforcing $n$-gram consistency constraints. Fuzzy dictionary matching compares predicted $n$-gram sets with a large lookup table of candidate identifiers and uses Dice similarity to rank candidates. A semantic reconstruction step leverages \ac{llm}-based scoring to refine predictions when textual plausibility is required. The combination of these strategies allows the attacker to either directly reconstruct plaintext identifiers or produce high-confidence candidate lists that can be cross-referenced against auxiliary sources.

\subsection{Hyperparameter Optimization and Model Selection}
A key innovation of the \ac{dea} is the extensive hyperparameter optimization (HPO) workflow. For each dataset, encoding scheme, overlap level, and drop-from strategy, the thesis conducts 125 optimization trials using Optuna. The search space spans architectural depth (1--3 layers), hidden dimension sizes (64--4096), dropout (0--0.5), activation functions (\texttt{relu}, \texttt{elu}, \texttt{selu}, \texttt{gelu}), optimizers (\texttt{Adam}, \texttt{AdamW}, \texttt{RMSprop}), learning rate schedules, batch sizes, and decision thresholds. Each trial trains for up to 20 epochs with early stopping (patience of five epochs, minimum validation-loss improvement of $10^{-4}$). The Dice coefficient on a held-out validation set guides the search. After HPO converges, the best-performing configuration is retrained on the combined training and validation data to maximize generalization before predictions on the test set are generated.

The architectural patterns discovered through HPO reveal encoding-specific preferences. For \ac{tmh}, shallow but extremely wide feedforward networks with 1024--2048 hidden units, moderate dropout around 0.25, \texttt{elu}/\texttt{selu} activations, and \texttt{AdamW} optimization consistently yield top performance. \ac{tsh} experiments favor slightly deeper networks with two hidden layers, stronger regularization, and cyclic learning rate schedules, indicating that the two-step hashing introduces more complex feature interactions. \ac{bf} encodings benefit from balanced activation functions and \texttt{RMSprop} with cyclic schedules, suggesting that their dense binary feature vectors respond well to adaptive yet oscillating learning rates. Across all schemes, small batch sizes (8 or 16) and near-maximum epoch counts underscore the need to fully exploit the scarce labeled data produced by the \ac{gma}.

\subsection{Experimental Design and Evaluation Protocol}
The evaluation spans 180 unique experiments covering three datasets (\texttt{fakename} with 1k--20k records, \texttt{euro\_person}, and \texttt{titanic\_full}), three encoding schemes, overlap ratios between 20\% and 90\%, and two drop-from strategies. ``DropFrom = Eve'' models optimistic scenarios in which Eve's auxiliary data is a strict subset of Alice's database, maximizing overlap quality. ``DropFrom = Both'' reflects more realistic deployments where both parties hold unique records, resulting in noisier graph structures and weaker training signals. For each configuration, the \ac{gma} supplies the training set, while the remaining encoded records serve as test data. Performance is reported in terms of structural prediction quality (precision, recall, F1-score, Dice) and downstream re-identification rate obtained after fuzzy and greedy reconstruction. Experiments in which the \ac{gma} fails to identify any matches are excluded because the \ac{dea} lacks labeled data in such cases.

To ensure comparability, encoding parameters mirror those used in contemporary \ac{pprl} studies. \ac{bf}s employ 1024-bit arrays with 10 hash functions, \ac{tmh} uses 1024 hash functions at 64-bit precision, and \ac{tsh} relies on 10 hash functions and 1000 columns before the second hashing stage. All models operate on overlapping 2-grams and use Dice similarity for graph alignment and evaluation. These standardized settings make the observed differences directly attributable to the encoding characteristics and overlap structure, rather than arbitrary parameter tuning.

\section{Empirical Findings}
\subsection{Tabulation MinHash}
Across all datasets, \ac{tmh} exhibits the strongest resilience in low-overlap scenarios but ultimately succumbs to \ac{dea}s when sufficient training data becomes available. On the smallest datasets (\texttt{fakename\_1k} and \texttt{fakename\_2k}), F1-scores rise from below 0.2 to around 0.73 as overlap increases from 0.4 to 0.8 under ``DropFrom = Eve'', yet re-identification remains negligible. The turning point occurs at \texttt{fakename\_5k}, where the \ac{dea} surpasses 0.85 F1-score and achieves the first non-zero re-identification rate (1.2\%) at 0.6 overlap. Larger datasets amplify this effect: \texttt{fakename\_10k} yields near-perfect structural reconstruction (F1 $\approx$ 0.93) and re-identification rates above 5\% for overlaps of 0.6 and higher, while \texttt{fakename\_20k} peaks near 12\% re-identification at 0.8 overlap. The realistic \texttt{euro\_person} dataset mirrors these trends with F1-scores above 0.9 and re-identification rates up to 6.85\%. These results demonstrate a pronounced non-linear relationship between structural accuracy and successful deanonymization: once F1-scores exceed roughly 0.9, small improvements translate into steep increases in re-identified individuals.

\subsection{Two-Step Hashing}
\ac{tsh} proves the most vulnerable encoding overall. Even on the modest \texttt{titanic\_full} dataset, F1-scores between 0.56 and 0.83 emerge for overlaps of 0.7--0.9, accompanied by consistent re-identifications once overlap exceeds 0.8. Synthetic \texttt{fakename} datasets show accelerated leakage: \texttt{fakename\_2k} already reaches F1 = 0.71 and a 1.3\% re-identification rate at 0.6 overlap under ``DropFrom = Eve''. Scaling to \texttt{fakename\_20k} under favorable overlaps produces F1-scores close to 0.99 and peak re-identification rates of 28.75\%, the highest observed across all experiments. The \texttt{euro\_person} dataset confirms the severity, with structural accuracy around 0.96 and re-identification up to 12.5\%. The extra hashing layer in \ac{tsh} appears insufficient to disrupt the statistical dependencies learned by the \ac{dea}, indicating that the chained representation still exposes predictable co-occurrence patterns. These findings suggest that \ac{tsh}, despite being proposed as a more secure successor to \ac{bf}s, becomes the weakest link when adversaries can marshal machine learning.

\subsection{Bloom Filter Encoding}
\ac{bf}s occupy a middle ground: they are more robust than \ac{tsh} but substantially weaker than \ac{tmh} once dataset size and overlap increase. On \texttt{fakename\_1k}, \ac{dea} performance remains near baseline, with F1-scores under 0.3 and no re-identifications. However, \texttt{fakename\_5k} already crosses the critical F1 = 0.8 threshold at 0.6 overlap under ``DropFrom = Eve'', leading to re-identification rates approaching 4\%. The larger \texttt{fakename} instances further erode privacy: \texttt{fakename\_20k} experiences average re-identification rates around 9\% with peaks near 15\% at 0.8 overlap. The \texttt{euro\_person} dataset shows similar vulnerability, attaining F1 = 0.94 and 7.6\% re-identification in the mixed drop-from scenario. Architectural analyses highlight that \ac{bf} experiments favor two-layer networks with balanced activations and \texttt{RMSprop} optimization, underscoring that even moderate architectural complexity is sufficient to unlock their structural leakage. These outcomes reframe \ac{bf}s from a ``best practice'' solution to a high-risk option in contexts where attackers can gather overlap samples.

\subsection{Aggregate Trends and Correlations}
Aggregating across all experiments reveals several consistent patterns. First, overlap size is the dominant driver of \ac{dea} success: every encoding exhibits a monotonic increase in both F1-score and re-identification rate as overlap grows, with the steepest gains between 0.4 and 0.6. Second, dataset size amplifies leakage because larger corpora produce richer training signals and more homogeneous distributions, enabling the neural model to generalize aggressively. Third, the drop-from strategy matters primarily at low overlaps; once overlap exceeds 0.6, the difference between ``DropFrom = Eve'' and ``DropFrom = Both'' narrows substantially, implying that realistic auxiliary noise does little to protect against a determined attacker. Finally, structural accuracy and re-identification exhibit a sigmoidal relationship: below F1 = 0.7, re-identification remains near zero; between 0.7 and 0.9, the leakage increases gradually; beyond 0.9, small F1 improvements can double or triple the number of compromised records. This characteristic informs defenders about the thresholds at which monitoring and countermeasures become urgent.

\section{Implications and Recommendations}
The evaluation establishes that all three encoding schemes are vulnerable to dataset extension once an attacker can harvest even a modest training set from a \ac{gma}. The key privacy implications are twofold. First, the traditional security argument that non-overlapping records remain safe no longer holds; \ac{dea}s can infer their $n$-gram structure with high fidelity and translate that into identifiable plaintext strings. Second, the availability of efficient HPO and neural toolkits lowers the barrier for adversaries. Training hundreds of models across 180 scenarios remained computationally feasible within academic resources, suggesting that well-funded attackers could automate the process.

To mitigate these risks, the thesis discusses several countermeasures. Increasing the entropy of encoding outputs through diffusion or salting can disrupt the deterministic mappings that the \ac{dea} exploits. Limiting overlap sizes, e.g., by enforcing partitioned linkage workflows or rate-limiting repeated queries, reduces the attacker's training data. Incorporating cryptographic techniques such as secure multi-party computation or homomorphic encryption can decouple similarity computation from direct exposure of encodings, albeit at higher computational cost. Finally, monitoring for abnormal re-identification patterns---for example, a sudden surge in fuzzy matches consistent with \ac{dea} predictions---can help detect ongoing attacks. Nevertheless, the study cautions that incremental tweaks to existing encodings are unlikely to suffice; a shift toward cryptographically rigorous linkage protocols may be necessary for high-stakes domains.

\section{Conclusion and Outlook}
This summary highlights the thesis' main contributions: formalizing the \ac{dea} threat model, implementing a comprehensive neural attack pipeline, and empirically demonstrating the vulnerability of widely used \ac{pprl} encodings across realistic datasets. The work shows that \ac{tsh} is the most brittle scheme under dataset extension, \ac{bf}s offer only moderate resistance, and \ac{tmh}, while comparatively resilient, still yields to sustained inference at scale. The attack achieves peak re-identification rates of 28.75\% and maintains average structural F1-scores above 0.6 across 180 experiments, far surpassing naive frequency baselines. These findings underscore that privacy assurances grounded solely in obscurity or limited overlap are no longer defensible.

Future research should pursue three directions. First, defenders need principled mechanisms to bound information leakage from similarity-preserving encodings, potentially by integrating differential privacy or cryptographic commitments. Second, subsequent attackers may extend the \ac{dea} with generative models, adversarial training, or transfer learning to accelerate inference across domains, necessitating proactive countermeasures. Third, policy and governance frameworks must adapt to require rigorous risk assessments before deploying \ac{pprl} solutions in sensitive contexts. By revealing the scope of the dataset extension threat, the thesis provides both a warning and a foundation for designing the next generation of privacy-preserving linkage systems.

\end{document}
